{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONE: Hello World\n",
    "# DONE: Crawl over stored Files\n",
    "# DONE: Operate Elastic Search Class\n",
    "# DONE: Populate wiki_evidence\n",
    "# DONE: How to manage a peristant volume\n",
    "# TODOs: Information Retrieval program, given search and rank sentences.\n",
    "# TODOs: Implement DELETE on Database Class\n",
    "# https://stackoverflow.com/questions/34621093/persist-elastic-search-data-in-docker-container\n",
    "# TODOs: Repurpose DrQA Elasticsearch Module\n",
    "# TODOs: Run Full-Evidence\n",
    "# TODOs: Nice Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17039\n"
     ]
    }
   ],
   "source": [
    "### CRAWL FILE-SYSTEM ###\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "knowledge_path = \"/Users/joshua.sheppard/wiki_extract_II\"\n",
    "\n",
    "def iter_filesys(path):\n",
    "    if os.path.isfile(path):\n",
    "        yield path\n",
    "\n",
    "    elif os.path.isdir(path):\n",
    "        for dir, _, filenames in os.walk(path):\n",
    "                for f in filenames:\n",
    "                    if not f.endswith('.DS_Store'):\n",
    "                        yield os.path.join(dir, f)\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Invalid path %s\" % path)\n",
    "\n",
    "kw_files = iter_filesys(knowledge_path)\n",
    "\n",
    "kw_sample = []\n",
    "for i in kw_files:\n",
    "    kw_sample.append(i)\n",
    "\n",
    "# print(kw_sample[:-3])\n",
    "print(len(kw_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### UPLOAD USING GENERATOR ###\n",
    "from multiprocessing import Pool\n",
    "from utils import get_contents\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# TODOs: Apply Paralell Multiprocess\n",
    "# def get_contents_(filename):\n",
    "#     \"\"\"Parse the contents of a file. Each line is a JSON encoded document.\"\"\"\n",
    "#     documents = []\n",
    "\n",
    "#     with open(filename) as f:\n",
    "#         for line in f:\n",
    "#             doc = json.loads(line)\n",
    "\n",
    "#             if doc[\"text\"] == \"\": continue\n",
    "#             if not doc: continue\n",
    "\n",
    "#             documents.append((doc['id'], doc[\"title\"], doc[\"text\"]))\n",
    "\n",
    "#     return documents\n",
    "\n",
    "# files = [f for f in kw_sample]\n",
    "\n",
    "# def generate_data():\n",
    "#     with tqdm(total=len(files)) as pbar:\n",
    "#         #for document in p.map(utils.get_contents_2, files):\n",
    "#         for file in files:\n",
    "#                 #yield get_contents_2(file)\n",
    "#                 docs = get_contents_(file)\n",
    "#                 for doc in docs:\n",
    "#                     yield(doc)\n",
    "\n",
    "#         pbar.update()\n",
    "\n",
    "# generate_data()\n",
    "# wiki_data = [i for i in generate_data()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# len(wiki_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge = wiki_data[0]\n",
    "# knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "files = [f for f in kw_sample]\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_contents_(filename):\n",
    "    \"\"\"Parse the contents of a file. Each line is a JSON encoded document.\"\"\"\n",
    "    documents = []\n",
    "\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            if doc[\"text\"] == \"\": continue\n",
    "            if not doc: continue\n",
    "\n",
    "            documents.append((doc['id'], doc[\"title\"], doc[\"text\"]))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def generate_data():\n",
    "    #with tqdm(total=len(files)) as pbar:\n",
    "        #for document in p.map(utils.get_contents_2, files):\n",
    "        for file in files:\n",
    "                #yield get_contents_2(file)\n",
    "                docs = get_contents_(file)\n",
    "                for doc in docs:\n",
    "                    yield(doc)\n",
    "\n",
    "        #pbar.update()\n",
    "\n",
    "import more_itertools\n",
    "def sentence_window(article, window=5, step=2): \n",
    "    \"\"\" Generates a list of sentences of sliding size = window \"\"\"\n",
    "    \n",
    "    sents = list(nlp(article).sents)\n",
    "    \n",
    "    if len(sents) == window:\n",
    "        yield str(sents)\n",
    "\n",
    "    for window in more_itertools.windowed(sents, n=window, step=2):\n",
    "        yield window\n",
    "\n",
    "    # for i in range(len(sents) - window + 1):\n",
    "    #     #print(sents[i:i + window])\n",
    "    #     #yield(sents[i:i + window])\n",
    "    #     yield(sents[i:i + window])\n",
    "\n",
    "# generate_data()\n",
    "# wiki_data_passages = []\n",
    "# count = 0\n",
    "\n",
    "pool = Pool(8)\n",
    "\n",
    "# def to_raw(string):\n",
    "#     return fr\"{string}\"\n",
    "\n",
    "import re\n",
    "def clean(passage):\n",
    "    passage = str(passage)\n",
    "    #passage = to_raw(passage)\n",
    "    passage.encode(\"unicode_escape\")\n",
    "    passage.replace('\"', '\"')\n",
    "    passage.strip()\n",
    "    passage = re.sub(\"\\n\", \"\", passage)\n",
    "    passage = re.sub('\"', \"'\", passage)\n",
    "    \n",
    "    return passage    \n",
    "    \n",
    "def passages(idx, source):\n",
    "    count = 0\n",
    "    with tqdm(total=len(files)) as pbar:\n",
    "        for i in pool.apply(generate_data):\n",
    "            count += 1\n",
    "            id, title, article = i\n",
    "\n",
    "            for window in sentence_window(article):\n",
    "                passage = \" \".join(clean(passage) for passage in window)\n",
    "\n",
    "                if len(passage) < 50: continue\n",
    "            \n",
    "                yield {\n",
    "                    \"_index\": idx,\n",
    "                    \"document\": {\n",
    "                        \"id\": id,\n",
    "                        \"source\": source,\n",
    "                        \"title\": title,\n",
    "                        \"text\": passage\n",
    "                        }\n",
    "                    }\n",
    "            \n",
    "            pbar.update()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/17039 [00:00<10:38, 26.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# 36.2 = 1000\n",
    "test = []\n",
    "\n",
    "count = 0\n",
    "for i in passages(idx=\"testing\", source=\"wikipedia\"):\n",
    "    count += 1\n",
    "    \n",
    "    if count > 100:\n",
    "            break\n",
    "\n",
    "    test.append(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': 'Synopsis. Witch &amp; Wizard. The novel follows fifteen-year old Wisty and her eighteen-year-old brother Whit in a dystopian future, where people can be brought up on charges of witchcraft. The teens and their parents are charged with witchcraft despite their claims that magic does not exist. They are taken from their parents when they begin to show magical abilities after receiving a book and drum stick.'}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': 'The novel follows fifteen-year old Wisty and her eighteen-year-old brother Whit in a dystopian future, where people can be brought up on charges of witchcraft. The teens and their parents are charged with witchcraft despite their claims that magic does not exist. They are taken from their parents when they begin to show magical abilities after receiving a book and drum stick. The children meet The One Who Is The One, the leader of the political party the New Order. The teens are sentenced to be executed once they turn eighteen years of age.'}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': \"They are taken from their parents when they begin to show magical abilities after receiving a book and drum stick. The children meet The One Who Is The One, the leader of the political party the New Order. The teens are sentenced to be executed once they turn eighteen years of age. Whit is reunited with his long lost girlfriend Celia, who reveals that she's a half-light spirit that exists on an alternate dimension called the Shadow land. She teaches the two of them how to travel to and from Shadow-land, through which they escape to a haven for persecuted children named Freeland.\"}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': \"The teens are sentenced to be executed once they turn eighteen years of age. Whit is reunited with his long lost girlfriend Celia, who reveals that she's a half-light spirit that exists on an alternate dimension called the Shadow land. She teaches the two of them how to travel to and from Shadow-land, through which they escape to a haven for persecuted children named Freeland. The siblings choose not to remain in the haven, as they wish to find their parents, and learn of a prophecy where they are Liberators mentioned in a prophecy about the end of the New Order. Wisty and Whit begin to plan to take down the regime and reluctantly accept help from Byron, a former classmate who had heavily taken part in their persecution and had been changed into a weasel by Wisty.\"}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': \"She teaches the two of them how to travel to and from Shadow-land, through which they escape to a haven for persecuted children named Freeland. The siblings choose not to remain in the haven, as they wish to find their parents, and learn of a prophecy where they are Liberators mentioned in a prophecy about the end of the New Order. Wisty and Whit begin to plan to take down the regime and reluctantly accept help from Byron, a former classmate who had heavily taken part in their persecution and had been changed into a weasel by Wisty. The trio eventually return to their home town, where they discover that the siblings' home has been demolished. There they discover that their parents can communicate with them through magic.\"}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': \"Wisty and Whit begin to plan to take down the regime and reluctantly accept help from Byron, a former classmate who had heavily taken part in their persecution and had been changed into a weasel by Wisty. The trio eventually return to their home town, where they discover that the siblings' home has been demolished. There they discover that their parents can communicate with them through magic. During this process their seemingly worthless book and drum stick are transformed into a magic wand and spell book. Byron is also changed back into a human and charged with looking after the siblings.\"}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': 'There they discover that their parents can communicate with them through magic. During this process their seemingly worthless book and drum stick are transformed into a magic wand and spell book. Byron is also changed back into a human and charged with looking after the siblings. The Gift. When Whit and Wisty were imprisoned by the wicked forces of the totalitarian regime known as the New Order, they were barely able to escape with their lives.'}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': 'Byron is also changed back into a human and charged with looking after the siblings. The Gift. When Whit and Wisty were imprisoned by the wicked forces of the totalitarian regime known as the New Order, they were barely able to escape with their lives. Now part of a hidden community of teens like themselves, Whit and Wisty have established themselves as leaders of the Resistance, willing to sacrifice anything to save kidnapped and imprisoned kids. Now the villainous leader of the New Order is just a breath away from the ability to control the forces of nature and to manipulate his citizens on the most profound level imaginable: through their minds.'}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': 'When Whit and Wisty were imprisoned by the wicked forces of the totalitarian regime known as the New Order, they were barely able to escape with their lives. Now part of a hidden community of teens like themselves, Whit and Wisty have established themselves as leaders of the Resistance, willing to sacrifice anything to save kidnapped and imprisoned kids. Now the villainous leader of the New Order is just a breath away from the ability to control the forces of nature and to manipulate his citizens on the most profound level imaginable: through their minds. There is only one more thing he needs to triumph in his evil quest: the Gifts of Whit and Wisty Allgood. And he will stop at nothing to seize them.'}},\n",
       " {'_index': 'testing',\n",
       "  'document': {'id': '61838078',\n",
       "   'source': 'wikipedia',\n",
       "   'title': 'Witch &amp; Wizard (series)',\n",
       "   'text': 'Now the villainous leader of the New Order is just a breath away from the ability to control the forces of nature and to manipulate his citizens on the most profound level imaginable: through their minds. There is only one more thing he needs to triumph in his evil quest: the Gifts of Whit and Wisty Allgood. And he will stop at nothing to seize them. The Fire. The siblings Whit and Wisty Allgood have every reason to want to take down nick, especially as he has taken everything and everyone from them.'}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(test)\n",
    "test[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# INIT OBJECTS\n",
    "\n",
    "PORT = \"http://localhost:9200\"\n",
    "INDEX_NAME = \"knowledge\"\n",
    "SOURCE = \"wikipedia\"\n",
    "errors_before_interrupt = 5\n",
    "refresh_index_after_insert = False\n",
    "max_insert_retries = 3\n",
    "yield_ok = False\n",
    "\n",
    "wiki_ev = Elasticsearch(\n",
    "    PORT,\n",
    "    #http_auth=(es_api_user, es_api_password)\n",
    "    retry_on_timeout=True,  # should timeout trigger a retry on different node?\n",
    ")\n",
    "\n",
    "wiki_ev.elastic_index = INDEX_NAME\n",
    "\n",
    "wiki_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/knowledge/_delete_by_query [status:200 duration:1.024s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 1020, 'timed_out': False, 'total': 30000, 'deleted': 30000, 'batches': 30, 'version_conflicts': 0, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DELETE ALL\n",
    "\n",
    "wiki_ev.delete_by_query(index=INDEX_NAME, query={\"match_all\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007889032363891602,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2ea19877ee4aedbd1b28b032d21137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008267879486083984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 17039,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a7fe9f62a04dbda67adbf60f01297c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://localhost:9200/knowledge/_bulk [status:200 duration:1.654s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# with tqdm(total=counta) as pbar:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m#for ok, result in parallel_bulk(es, passages(idx=INDEX_NAME, source=SOURCE), chunk_size=chunk_size, request_timeout=60*3):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m successes \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m ok, result \u001b[39min\u001b[39;00m tqdm(streaming_bulk(es, index\u001b[39m=\u001b[39mINDEX_NAME, actions\u001b[39m=\u001b[39mpassages(idx\u001b[39m=\u001b[39mINDEX_NAME, source\u001b[39m=\u001b[39mSOURCE), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                         chunk_size\u001b[39m=\u001b[39mchunk_size, request_timeout\u001b[39m=\u001b[39m\u001b[39m60\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m, max_retries\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mif\u001b[39;00m ok \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m             logging\u001b[39m.\u001b[39merror(\u001b[39m'\u001b[39m\u001b[39mFailed to import data\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/retriever/vs_env/lib/python3.10/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/retriever/vs_env/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/retriever/vs_env/lib/python3.10/site-packages/elasticsearch/helpers/actions.py:422\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[0;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m bulk_data: List[\n\u001b[1;32m    416\u001b[0m     Union[\n\u001b[1;32m    417\u001b[0m         Tuple[_TYPE_BULK_ACTION_HEADER],\n\u001b[1;32m    418\u001b[0m         Tuple[_TYPE_BULK_ACTION_HEADER, _TYPE_BULK_ACTION_BODY],\n\u001b[1;32m    419\u001b[0m     ]\n\u001b[1;32m    420\u001b[0m ]\n\u001b[1;32m    421\u001b[0m bulk_actions: List[\u001b[39mbytes\u001b[39m]\n\u001b[0;32m--> 422\u001b[0m \u001b[39mfor\u001b[39;00m bulk_data, bulk_actions \u001b[39min\u001b[39;00m _chunk_actions(\n\u001b[1;32m    423\u001b[0m     \u001b[39mmap\u001b[39m(expand_action_callback, actions), chunk_size, max_chunk_bytes, serializer\n\u001b[1;32m    424\u001b[0m ):\n\u001b[1;32m    426\u001b[0m     \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_retries \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    427\u001b[0m         to_retry: List[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/PycharmProjects/retriever/vs_env/lib/python3.10/site-packages/elasticsearch/helpers/actions.py:232\u001b[0m, in \u001b[0;36m_chunk_actions\u001b[0;34m(actions, chunk_size, max_chunk_bytes, serializer)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39mSplit actions into chunks by number or size, serialize them into strings in\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mthe process.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m chunker \u001b[39m=\u001b[39m _ActionChunker(\n\u001b[1;32m    230\u001b[0m     chunk_size\u001b[39m=\u001b[39mchunk_size, max_chunk_bytes\u001b[39m=\u001b[39mmax_chunk_bytes, serializer\u001b[39m=\u001b[39mserializer\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[39mfor\u001b[39;00m action, data \u001b[39min\u001b[39;00m actions:\n\u001b[1;32m    233\u001b[0m     ret \u001b[39m=\u001b[39m chunker\u001b[39m.\u001b[39mfeed(action, data)\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m ret:\n",
      "\u001b[1;32m/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb Cell 11\u001b[0m in \u001b[0;36mpassages\u001b[0;34m(idx, source)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mid\u001b[39m, title, article \u001b[39m=\u001b[39m i\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mfor\u001b[39;00m window \u001b[39min\u001b[39;00m sentence_window(article):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     passage \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(clean(passage) \u001b[39mfor\u001b[39;00m passage \u001b[39min\u001b[39;00m window)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(passage) \u001b[39m<\u001b[39m \u001b[39m50\u001b[39m: \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;32m/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb Cell 11\u001b[0m in \u001b[0;36msentence_window\u001b[0;34m(article, window, step)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentence_window\u001b[39m(article, window\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m): \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m\"\"\" Generates a list of sentences of sliding size = window \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     sents \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nlp(article)\u001b[39m.\u001b[39msents)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sents) \u001b[39m==\u001b[39m window:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/retriver/construct_knowledgebase.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mstr\u001b[39m(sents)\n",
      "File \u001b[0;32m~/PycharmProjects/retriever/vs_env/lib/python3.10/site-packages/spacy/language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1020\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1022\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch.helpers import streaming_bulk, parallel_bulk\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def sentence_window(article, window=3, step=2): \n",
    "#     \"\"\" Generates a list of sentences of sliding size = window \"\"\"\n",
    "    \n",
    "#     sents = list(nlp(article).sents)\n",
    "    \n",
    "#     if len(sents) == window:\n",
    "#         yield str(sents)\n",
    "\n",
    "#     for i in range(0, len(sents)):\n",
    "#         yield(str(sents[i:i + window]))\n",
    "\n",
    "# def load_data(docs, idx_):\n",
    "#     \"\"\" Generates an evidence document to be inserted into ES Index \"\"\"\n",
    "#     for doc in docs:\n",
    "#         idx, title, text = doc\n",
    "\n",
    "#         for paragraph in sentence_window(text):\n",
    "#             yield {\n",
    "#                 \"_index\": idx_,\n",
    "#                 \"document\": {\n",
    "#                     \"id\": idx,\n",
    "#                     \"title\": title,\n",
    "#                     \"text\": paragraph\n",
    "#                 }\n",
    "#             }\n",
    "\n",
    "### TEST ###\n",
    "# test = []\n",
    "# id, title, text = knowledge\n",
    "\n",
    "# kw = sentence_window(knowledge[2])\n",
    "# for i in kw:\n",
    "#     print(i)\n",
    "\n",
    "es = wiki_ev\n",
    "errors_count = 0\n",
    "\n",
    "# TODOs: Tune chunk size\n",
    "chunk_size = 10000\n",
    "counta = len(files)//chunk_size\n",
    "\n",
    "# with tqdm(total=counta) as pbar:\n",
    "    #for ok, result in parallel_bulk(es, passages(idx=INDEX_NAME, source=SOURCE), chunk_size=chunk_size, request_timeout=60*3):\n",
    "successes = 0\n",
    "for ok, result in tqdm(streaming_bulk(es, index=INDEX_NAME, actions=passages(idx=INDEX_NAME, source=SOURCE), \n",
    "                        chunk_size=chunk_size, request_timeout=60*3, max_retries=3)):\n",
    "    if ok is not True:\n",
    "            logging.error('Failed to import data')\n",
    "            logging.error(str(result))\n",
    "            errors_count += 1\n",
    "\n",
    "            if errors_count == errors_before_interrupt:\n",
    "                logging.fatal('Too many import errors, exiting with error code')\n",
    "                exit(1)\n",
    "    \n",
    "    successes += ok\n",
    "\n",
    "    #pbar.update()   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = \"government emails privacy\"\n",
    "\n",
    "def search(query, k=5):\n",
    "    results = es.search(\n",
    "        index = es.elastic_index,\n",
    "        body= {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"document.text\": query,\n",
    "        }}})\n",
    "\n",
    "    hits = results[\"hits\"][\"hits\"]\n",
    "    doc_ids = [row['_source'][\"document\"][\"id\"] for row in hits]\n",
    "\n",
    "    print(results)\n",
    "    return (hits, doc_ids)\n",
    "\n",
    "\n",
    "test = search(query, k=2)[0][0][\"_source\"][\"document\"][\"text\"]\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# from elasticsearch import helpers\n",
    "# from elasticsearch.helpers import streaming_bulk, parallel_bulk\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# \"https://stackoverflow.com/questions/67522617/elasticsearch-bulk-insert-w-python-socket-timeout-error#:~:text=The%20connection%20to%20elasticsearch%20has,be%20handled%20as%20an%20error.\"\n",
    "\n",
    "# \"https://github.com/elastic/elasticsearch-py/issues/297\"\n",
    "\n",
    "# def load_data(docs, idx_):\n",
    "#     for doc in docs:\n",
    "#         idx, title, text = doc\n",
    "#         doc_ = {\"id\": idx, \"title\": title, \"text\": text}\n",
    "\n",
    "#         yield {\n",
    "#             \"_index\": idx_,\n",
    "#             \"document\": {\n",
    "#                 \"id\": idx,\n",
    "#                 \"title\": title,\n",
    "#                 \"text\": text\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "# # TODOs: Utils, Duration Function as Decorator\n",
    "# es = wiki_ev\n",
    "# errors_count = 0\n",
    "\n",
    "# # TODOs: Increase Chunk Size, with extended Timeout + handeling\n",
    "# # TODOs: Experiment-Check with Yield OK\n",
    "# for ok, result in parallel_bulk(es, load_data(wiki_data, \"wiki_evidence\"), chunk_size=500, request_timeout=60*3):\n",
    "#     if ok is not True:\n",
    "#             logging.error('Failed to import data')\n",
    "#             logging.error(str(result))\n",
    "#             errors_count += 1\n",
    "\n",
    "#             if errors_count == errors_before_interrupt:\n",
    "#                 logging.fatal('Too many import errors, exiting with error code')\n",
    "#                 exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def sentence_window(article, window=3, step=2): \n",
    "#     \"\"\" Generates a list of sentences of sliding size = window \"\"\"\n",
    "    \n",
    "#     sents = list(nlp(article).sents)\n",
    "    \n",
    "#     if len(sents) == window:\n",
    "#         yield sents\n",
    "\n",
    "#     for i in range(0, len(sents)):\n",
    "#         yield(sents[i:i + window])\n",
    "\n",
    "# def load_data(docs, idx_):\n",
    "#     \"\"\" Generates an evidence document to be inserted into ES Index \"\"\"\n",
    "#     for doc in docs:\n",
    "#         idx, title, text = doc\n",
    "\n",
    "#         for paragraph in sentence_window(text):\n",
    "#             yield {\n",
    "#                 \"_index\": idx_,\n",
    "#                 \"document\": {\n",
    "#                     \"id\": idx,\n",
    "#                     \"title\": title,\n",
    "#                     \"text\": paragraph\n",
    "#                 }\n",
    "#             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# from elasticsearch import helpers\n",
    "# from elasticsearch.helpers import streaming_bulk\n",
    "# from tqdm import tqdm\n",
    "#\n",
    "# \"https://stackoverflow.com/questions/67522617/elasticsearch-bulk-insert-w-python-socket-timeout-error#:~:text=The%20connection%20to%20elasticsearch%20has,be%20handled%20as%20an%20error.\"\n",
    "#\n",
    "# def load_data(docs, idx_):\n",
    "#     for doc in docs:\n",
    "#         idx, title, text = doc\n",
    "#         doc_ = {\"id\": idx, \"title\": title, \"text\": text}\n",
    "#\n",
    "#         yield {\n",
    "#             \"_index\": idx_,\n",
    "#             \"document\": doc\n",
    "#         }\n",
    "#\n",
    "# # TODOs: Utils, Duration Function as Decorator\n",
    "# es = wiki_ev\n",
    "# #helpers.bulk(es, load_data(wiki_data, \"wiki_evidence\"), raise_on_error=False, chunk_size=500)\n",
    "# errors_count = 0\n",
    "# # TODOs: Increase Chunk Size, with extended Timeout + handelling\n",
    "# for ok, result in streaming_bulk(es, load_data(wiki_data, \"wiki_evidence\"), chunk_size=500, request_timeout=60*3, yield_ok=yield_ok, refresh=refresh_index_after_insert):\n",
    "#     if ok is not True:\n",
    "#             logging.error('Failed to import data')\n",
    "#             logging.error(str(result))\n",
    "#             errors_count += 1\n",
    "#\n",
    "#             if errors_count == errors_before_interrupt:\n",
    "#                 logging.fatal('Too many import errors, exiting with error code')\n",
    "#                 exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for article in wiki_data:\n",
    "#     _id, title, text = article\n",
    "#     doc = {\"id\": _id, \"title\": title, \"text\": text}\n",
    "#\n",
    "#     wiki_ev.add_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/elastic/elasticsearch-py/issues/297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### DOCUMENT IMPORT: FULL-TEXTS ###\n",
    "# from multiprocessing import Pool\n",
    "# # from utils import get_contents\n",
    "# import utils\n",
    "# from tqdm import tqdm\n",
    "# import json\n",
    "\n",
    "# # TODOs: USE A GENERATOR OBJECT\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "#\n",
    "# p = Pool(8)\n",
    "# files = [f for f in kw_sample]\n",
    "#\n",
    "# count = 0\n",
    "# test = []\n",
    "# with tqdm(total=len(files)) as pbar:\n",
    "#     for documents in p.map(utils.get_contents_2, files):\n",
    "#             for doc in documents:\n",
    "#                 _id, title, text = doc\n",
    "#\n",
    "#             # count += 1\n",
    "#             # doc_ = {\"id\": _id, \"title\": title, \"text\": text}\n",
    "#\n",
    "#             #wiki_ev.add_doc(doc)\n",
    "#             test.append(doc_)\n",
    "#     pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "\n",
    "# # INIT OBJECT\n",
    "\n",
    "# # TODOs: Persist a Generator Object\n",
    "# PORT = \"http://localhost:9200\"\n",
    "# INDEX_NAME = \"wiki_evidence\"\n",
    "# errors_before_interrupt = 5\n",
    "# refresh_index_after_insert = False\n",
    "# max_insert_retries = 3\n",
    "# yield_ok = False\n",
    "\n",
    "# wiki_ev = Elasticsearch(\n",
    "#     PORT,\n",
    "#     #http_auth=(es_api_user, es_api_password)\n",
    "#     retry_on_timeout=True,  # should timeout trigger a retry on different node?\n",
    "# )\n",
    "\n",
    "# wiki_ev.elastic_index = INDEX_NAME\n",
    "\n",
    "# wiki_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### DOCUMENT IMPORT: SEGMENTED-TEXTS ###\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# # from utils import get_contents\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # p = Pool(8)\n",
    "# # files = [f for f in kw_sample]\n",
    "# #\n",
    "# # count = 0\n",
    "# # test = []\n",
    "# # with tqdm(total=len(files)) as pbar:\n",
    "# #     for documents in p.map(get_contents, files):\n",
    "# #         for doc in documents:\n",
    "# #             _id, title, text = doc\n",
    "# #\n",
    "# #             count += 1\n",
    "# #             doc_ = {\"id\": _id, \"title\": title, \"text\": text}\n",
    "# #\n",
    "# #             #wiki_ev.add_doc(doc)\n",
    "# #             test.append(doc)\n",
    "# #     pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### QUERY DB ###\n",
    "# import elastic_db\n",
    "# # from elastic_db import ElasticDB\n",
    "# #\n",
    "# # # Params\n",
    "# # PORT = \"http://localhost:9200\"\n",
    "# # INDEX = \"wiki_evidence\"\n",
    "# # DOC = \"evidence\"\n",
    "# #\n",
    "# # # Init Elasticsearch DB\n",
    "# # wiki_ev_ = ElasticDB(elastic_port=PORT, elastic_index=INDEX, elastic_doc=DOC)\n",
    "# #\n",
    "# # results = wiki_ev_.search(\"exploitation a wider public debate indecency adult\")\n",
    "# # results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### SQLITE LOAD ###\n",
    "# from multiprocessing import Pool\n",
    "# import utils\n",
    "# from tqdm import tqdm\n",
    "# import sqlite3\n",
    "\n",
    "# import spacy\n",
    "# import uuid\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def paragraphs(document):\n",
    "#     start = 0\n",
    "#     document = nlp(document)\n",
    "#     passages = []\n",
    "#     for token in document:\n",
    "#         if token.is_space and token.text.count(\"\\n\") > 1:\n",
    "#             yield document[start:token.i]\n",
    "#             start = token.i\n",
    "#     yield document[start:]\n",
    "\n",
    "\n",
    "# def get_contents(filename):\n",
    "#     \"\"\"Parse the contents of a file. Each line is a JSON encoded document.\"\"\"\n",
    "#     documents = []\n",
    "\n",
    "#     with open(filename) as f:\n",
    "#         for line in f:\n",
    "#             doc = json.loads(line)\n",
    "\n",
    "#             if doc[\"text\"] == \"\": continue\n",
    "#             if not doc: continue\n",
    "\n",
    "#             passages = [str(i) for i in paragraphs(doc[\"text\"])][0].split(\"\\n\")\n",
    "\n",
    "#             for passage in passages:\n",
    "#                 if len(passage) < 50:\n",
    "#                     continue\n",
    "\n",
    "#                 documents.append((str(uuid.uuid4()).replace('-',''), doc['id'], doc[\"title\"], passage))\n",
    "\n",
    "#     return documents\n",
    "\n",
    "# save_path = \"../data/wiki_evidence.db\"\n",
    "#\n",
    "# p = Pool(8)\n",
    "# files = [f for f in kw_sample]\n",
    "#\n",
    "# conn = sqlite3.connect(save_path)\n",
    "# c = conn.cursor()\n",
    "#\n",
    "# documents = \"documents\"\n",
    "# c.execute(f\"CREATE TABLE documents (id PRIMARY KEY, id_, title, text);\")\n",
    "#\n",
    "# count = 0\n",
    "# step = 100\n",
    "# batches = [files[i:i + step] for i in range(0, len(files), step)]\n",
    "#\n",
    "# for i, batch in enumerate(batches):\n",
    "#     logger.info(f\"[.... Batch #{i} .....]\")\n",
    "#     with tqdm(total=len(batch)) as pbar:\n",
    "#         for document in tqdm(p.imap_unordered(get_contents, files)):\n",
    "#             count += 1\n",
    "#             for content in document:\n",
    "#                 # _id, title, passage = content\n",
    "#                 c.executemany(\"INSERT INTO documents VALUES (?,?,?,?)\", (content,))\n",
    "#\n",
    "#         pbar.update()\n",
    "#         logger.info(f\"[Uploaded {count} documents]\")\n",
    "#\n",
    "# conn.commit()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0, 1, 2]\n",
      "2\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# seq = [0, 1, 2, 3, 4, 5]\n",
    "# window_size = 3\n",
    "# step = 2\n",
    "\n",
    "# # steps = 0, 2, 4 \n",
    "\n",
    "# for i in range(0, len(seq) - window_size + 1, step):\n",
    "#     print(i)\n",
    "#     # print(i + window_size)\n",
    "#     if i + window_size > len(seq):\n",
    "#         # print(window_size)\n",
    "#         window_size = i + window_size - len(seq)\n",
    "    \n",
    "#     print(seq[i: i + window_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(seq), 2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 2), (2, 3, 4), (4, 5, None)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [0,1,2,3,4,5]\n",
    "\n",
    "import more_itertools\n",
    "\n",
    "test = more_itertools.windowed(seq,n=3, step=2)\n",
    "list(more_itertools.windowed(seq,n=3, step=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('vs_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f570427228f404ca5e5bd69d7c4f81f4bbf047c681baf5cde3c9d0636c1aef27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
